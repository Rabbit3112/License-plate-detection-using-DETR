{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations Train1 (License Plate Detection):\n",
      "    img_id  ymin  xmin  ymax  xmax\n",
      "0    1.jpg   276    94   326   169\n",
      "1   10.jpg   311   395   344   444\n",
      "2  100.jpg   406   263   450   434\n",
      "3  101.jpg   283   363   315   494\n",
      "4  102.jpg   139    42   280   222\n",
      "\n",
      "Annotations Train2 (License Plate Recognition):\n",
      "    img_id      text\n",
      "0    0.jpg  117T3989\n",
      "1    1.jpg  128T8086\n",
      "2   10.jpg   94T3458\n",
      "3  100.jpg  133T6719\n",
      "4  101.jpg   68T5979\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Paths to the CSV files\n",
    "detection_train_csv_path = \"D:\\\\Projects\\\\soulpage proj\\\\Licplatesdetection_train.csv\"\n",
    "recognition_train_csv_path = \"D:\\\\Projects\\\\soulpage proj\\\\Licplatesrecognition_train.csv\"\n",
    "\n",
    "# Load the CSV files\n",
    "annotations_train1 = pd.read_csv(detection_train_csv_path)\n",
    "annotations_train2 = pd.read_csv(recognition_train_csv_path)\n",
    "\n",
    "# Display first few annotations to understand the data structure\n",
    "print(\"Annotations Train1 (License Plate Detection):\")\n",
    "print(annotations_train1.head())\n",
    "print(\"\\nAnnotations Train2 (License Plate Recognition):\")\n",
    "print(annotations_train2.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "License Plate Detection Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_20152\\1740265282.py:11: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  image = cv2.imread(\"D:\\siva resume_coe\\recent\\29.jpg\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "\n",
    "# Assuming the columns in annotations_train1 are ['filename', 'ymin', 'xmin', 'ymax', 'xmax']\n",
    "annotations_train1['bbox'] = annotations_train1.apply(\n",
    "    lambda row: [row['xmin'], row['ymin'], row['xmax'] - row['xmin'], row['ymax'] - row['ymin']],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "class LicensePlateDataset(Dataset):\n",
    "    def __init__(self, img_dir, annotations, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.annotations = annotations\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[idx]['filename'])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n",
    "        bbox = self.annotations.iloc[idx]['bbox']\n",
    "        bbox = np.array(bbox, dtype=np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, bbox\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224))\n",
    "])\n",
    "\n",
    "train1_images_path = 'D:\\\\Projects\\\\soulpage proj\\\\train1_images'  # Update with the actual image directory path\n",
    "train_dataset = LicensePlateDataset(train1_images_path, annotations_train1, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "License Plate Recognition Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterRecognitionDataset(Dataset):\n",
    "    def __init__(self, img_dir, annotations, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.annotations = annotations\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[idx]['filename'])\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "        image = np.expand_dims(image, axis=2) / 255.0\n",
    "        label = self.annotations.iloc[idx]['text']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "transform_cr = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train2_images_path = 'D:\\\\Projects\\\\soulpage proj\\\\train2_images'  # Update with the actual image directory path\n",
    "train_dataset_cr = CharacterRecognitionDataset(train2_images_path, annotations_train2, transform=transform_cr)\n",
    "train_loader_cr = DataLoader(train_dataset_cr, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DetrForObjectDetection, DetrImageProcessor\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Load the model and processor\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=10,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Custom collate function\n",
    "def collate_fn(batch):\n",
    "    pixel_values = [item[0] for item in batch]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "\n",
    "    targets = [{\"boxes\": torch.tensor([item[1]])} for item in batch]\n",
    "\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": targets}\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "License Plate Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "\n",
    "# Load the model and processor\n",
    "model_cr = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "processor_cr = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args_cr = TrainingArguments(\n",
    "    output_dir=\"./results_cr\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Custom collate function for character recognition\n",
    "def collate_fn_cr(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    pixel_values = torch.stack(images)\n",
    "    labels = list(labels)\n",
    "\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# Trainer setup for character recognition\n",
    "trainer_cr = Trainer(\n",
    "    model=model_cr,\n",
    "    args=training_args_cr,\n",
    "    data_collator=collate_fn_cr,\n",
    "    train_dataset=train_dataset_cr,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer_cr.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
